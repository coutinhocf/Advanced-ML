This is the first assgiment for the Coursera course "Advanced Machine Learning and Signal Processing"

The first step is to insert the credentials to the Apache CouchDB / Cloudant database where your sensor data ist stored to.

In the project's overview tab of this project just select "Add to project"->Connection
From the section "Your service instances in IBM Cloud" select your cloudant database and click on "create"
Now click in the empty cell below labeled with "your cloudant credentials go here"
Click on the "10-01" symbol top right and selecrt the "Connections" tab
Find your data base connection and click on "Insert to code"
Done, just execute all cells one after the other and you are done - just note that in the last one you have to update your email address (the one you've used for coursera) and obtain a submittion token, you get this from the programming assingment directly on coursera.

In [3]:
# The code was removed by DSX for sharing.
In [4]:
spark = SparkSession\
    .builder\
    .appName("Cloudant Spark SQL Example in Python using temp tables")\
    .config("cloudant.host",credentials_1['custom_url'].split('@')[1])\
    .config("cloudant.username", credentials_1['username'])\
    .config("cloudant.password",credentials_1['password'])\
    .getOrCreate()
In [5]:
df=spark.read.load('shake', "org.apache.bahir.cloudant")

df.createOrReplaceTempView("df")
spark.sql("SELECT * from df").show()
+-----+--------+------+------+------+--------------------+--------------------+
|CLASS|SENSORID|     X|     Y|     Z|                 _id|                _rev|
+-----+--------+------+------+------+--------------------+--------------------+
|    0|asdfghjk|  0.35|  0.35|  0.35|051264651086a8831...|1-1fa237bd42ed062...|
|    0|asdfghjk| -0.09| -0.09| -0.09|051264651086a8831...|1-e386bfccaf26de2...|
|    0|asdfghjk| -0.19| -0.19| -0.19|051264651086a8831...|1-9c90bfb8b76e725...|
|    0|asdfghjk|  0.94|  0.94|  0.94|051264651086a8831...|1-5c7a0d3e4d3f371...|
|    0|asdfghjk| 11.54| 11.54| 11.54|051264651086a8831...|1-a893cab2063d12c...|
|    0|asdfghjk|-34.74|-34.74|-34.74|051264651086a8831...|1-bfbf2a62007a1fc...|
|    0|asdfghjk|  1.93|  1.93|  1.93|051264651086a8831...|1-25d6c03479cfba4...|
|    0|asdfghjk|  3.16|  3.16|  3.16|051264651086a8831...|1-f09d28c95bc7736...|
|    0|asdfghjk|  3.77|  3.77|  3.77|051264651086a8831...|1-dc7ad63ba956165...|
|    0|asdfghjk|  3.37|  3.37|  3.37|051264651086a8831...|1-548e0def90152ec...|
|    0|asdfghjk|  7.09|  7.09|  7.09|051264651086a8831...|1-ad8ab3fb921c856...|
|    0|asdfghjk| -0.96| -0.96| -0.96|051264651086a8831...|1-bbc2a411b1cc61b...|
|    0|asdfghjk| -0.96| -0.96| -0.96|051264651086a8831...|1-bbc2a411b1cc61b...|
|    0|asdfghjk|  2.28|  2.28|  2.28|051264651086a8831...|1-0d80ae29b7d161c...|
|    0|asdfghjk|  1.79|  1.79|  1.79|051264651086a8831...|1-73afc8a2c3a1f66...|
|    0|asdfghjk|  1.52|  1.52|  1.52|051264651086a8831...|1-1d8a4df963f6c57...|
|    0|asdfghjk|  1.29|  1.29|  1.29|051264651086a8831...|1-68942cf4825d151...|
|    0|asdfghjk|  1.23|  1.23|  1.23|051264651086a8831...|1-2578cf4df033520...|
|    0|asdfghjk|  1.24|  1.24|  1.24|051264651086a8831...|1-a6a6c29a47dd341...|
|    0|asdfghjk|  1.16|  1.16|  1.16|051264651086a8831...|1-d2b9fb4163bcde3...|
+-----+--------+------+------+------+--------------------+--------------------+
only showing top 20 rows

In [10]:
!rm -Rf a2_m1.parquet
In [11]:
df = df.repartition(1)
df.write.json('a2_m1.json')
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:

/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    318                     "An error occurred while calling {0}{1}{2}.\n".
--> 319                     format(target_id, ".", name), value)
    320             else:

Py4JJavaError: An error occurred while calling o177.json.
: org.apache.spark.sql.AnalysisException: path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sed3-dfaacfbd5b7e86-34fe57f9860b/notebook/work/a2_m1.json already exists.;
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:80)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)
	at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:484)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:520)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:215)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:198)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:473)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:90)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:55)
	at java.lang.reflect.Method.invoke(Method.java:508)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:811)


During handling of the above exception, another exception occurred:

AnalysisException                         Traceback (most recent call last)
<ipython-input-11-a92573790cf4> in <module>()
      1 df = df.repartition(1)
----> 2 df.write.json('a2_m1.json')

/usr/local/src/spark21master/spark/python/pyspark/sql/readwriter.py in json(self, path, mode, compression, dateFormat, timestampFormat)
    616         self._set_opts(
    617             compression=compression, dateFormat=dateFormat, timestampFormat=timestampFormat)
--> 618         self._jwrite.json(path)
    619 
    620     @since(1.4)

/usr/local/src/spark21master/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1131         answer = self.gateway_client.send_command(command)
   1132         return_value = get_return_value(
-> 1133             answer, self.gateway_client, self.target_id, self.name)
   1134 
   1135         for temp_arg in temp_args:

/usr/local/src/spark21master/spark/python/pyspark/sql/utils.py in deco(*a, **kw)
     67                                              e.java_exception.getStackTrace()))
     68             if s.startswith('org.apache.spark.sql.AnalysisException: '):
---> 69                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)
     70             if s.startswith('org.apache.spark.sql.catalyst.analysis'):
     71                 raise AnalysisException(s.split(': ', 1)[1], stackTrace)

AnalysisException: 'path file:/gpfs/global_fs01/sym_shared/YPProdSpark/user/sed3-dfaacfbd5b7e86-34fe57f9860b/notebook/work/a2_m1.json already exists.;'
In [28]:
!rm -f rklib.py
!wget https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py
--2018-05-27 12:24:50--  https://raw.githubusercontent.com/romeokienzler/developerWorks/master/coursera/ai/rklib.py
Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133
Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2029 (2.0K) [text/plain]
Saving to: ‘rklib.py’

100%[======================================>] 2,029       --.-K/s   in 0s      

2018-05-27 12:24:50 (20.2 MB/s) - ‘rklib.py’ saved [2029/2029]

In [29]:
!zip -r a2_m1.json.zip a2_m1.json
updating: a2_m1.parquet/ (stored 0%)
updating: a2_m1.parquet/_SUCCESS (stored 0%)
updating: a2_m1.parquet/._SUCCESS.crc (stored 0%)
  adding: a2_m1.parquet/.part-00000-8dedcb0a-d209-4757-adbd-0b6ff434e06d.snappy.parquet.crc (stored 0%)
  adding: a2_m1.parquet/part-00000-8dedcb0a-d209-4757-adbd-0b6ff434e06d.snappy.parquet (deflated 53%)
In [30]:
!base64 a2_m1.json.zip > a2_m1.json.zip.base64
